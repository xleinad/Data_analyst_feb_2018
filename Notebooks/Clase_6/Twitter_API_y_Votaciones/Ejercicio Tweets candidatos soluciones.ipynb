{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio candidatos "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En días pasados el ex Secretario de Educación Pública y coordinador de campaña del precandidaro del PRI publico el \n",
    "Siguiente tweet:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tweet.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En política , muchas veces , los candidatos tratan de definir su agenda y su propia personalidad a partir de una contraposición, es decir establecen o definen lo que son a partir de lo que NO son.\n",
    "Por lo anterior es interesante revisar a partir de una muestra de tweets de las cuentas de Meade y López si la afirmación de Nuño es cierta y si podemos identificar con claridad términos que nos hablen de un discurso específico.\n",
    "\n",
    "Objetivos :\n",
    "Aprender a tokenizar y limpiar los tweets, graficar y listar las palabras con mayor frecuencia en  un listado.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En los archivos  tweet100AMLO.csv y tweet100MEADE.csv se guardó un listado de 100 tweets publicados en las cuentas de cada uno de los candidatos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.- Importe las librerias necesarias para analizar el archivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "%pylab inline\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import twitter\n",
    "import sys\n",
    "import tweepy \n",
    "from tweepy.auth import OAuthHandler\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 2.-Abra el archivo y conviertalo en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos = open('tweet100AMLO.csv','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataSet2= pd.read_csv('tweet100AMLO.csv', encoding='utf-8'.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataSet2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.-Genere los tokens de los tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt=TweetTokenizer(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataSet2['Tokenslimpios2']= DataSet2['Tokenslimpios'].apply(tt.tokenize)\n",
    "DataSet2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4.-Limpie los términos más comunes (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('spanish')\n",
    "print stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataSet2[\"Tokenslimpios3\"] = DataSet2[\"Tokenslimpios2\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "DataSet2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5.-Limpie los signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataSet2[\"Tokenslimpios\"] = DataSet2[\"Tokens3\"].apply(remove_punctuations)\n",
    "DataSet2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 6.-Imprima las frecuencias de los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list = DataSet2[\"Tokenslimpios2\"].tolist()\n",
    "uniqueVals = np.unique(my_list)\n",
    "print uniqueVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    df = pd.DataFrame(my_list)\n",
    "    df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot = df.apply(pd.value_counts)\n",
    "print pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot =pd.DataFrame(pivot)\n",
    "pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.-Grafique las 10 palabras  más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=pivot.sum(axis=1).sort_values(ascending=False).head(10).plot.bar()\n",
    "print words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8.-Compare su gráfica con la de el otro candidato, ¿Qué  hallazgos identificas?\n",
    "Crees que estos  hallazgos se sostendrán ampliando la cantidad de tweets que tenemos?\n",
    "Ampliemos la información analizando ahora una muestra de 4020 tweets de cada uno de los candidatos, para ello utilice los archivos\n",
    "\n",
    "tweet4000AMLO.csv y tweet400MEADE.csv\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
